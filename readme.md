# Wikipedia Deaths Watcher

Скрипт для отслеживания появления новых записей на странице Википедии
со списком умерших и отправки уведомлений по email.

---

## Что делает скрипт

- периодически проверяет страницу Википедии со списком умерших;
- определяет появление новых персон;
- формирует уведомление с:
    - именем человека;
    - ссылкой на русскоязычную Википедию, если такая статья существует;
    - иначе — ссылкой из исходной страницы;
    - первым абзацем статьи без ссылок, сносок и ударений;
- сохраняет состояние между запусками;
- демонстрирует содержимое уведомлений в debug-выводе;
- не использует cron или сторонние планировщики.

---

## Установка и запуск

### Требования

- Python 3.9+
- pip

### Установка зависимостей

```bash
pip install -r requirements.txt
```

---

## Настройка отправки email-уведомлений

В проекте реализована отправка уведомлений через SMTP. Механизм является опциональным и не влияет на основную логику.

### Базовая настройка

В config.py необходимо указать:

```python
SMTP_HOST = "smtp.gmail.com"
SMTP_PORT = 587
SMTP_USER = "your_email@gmail.com"
SMTP_PASS = "app_password"
TO_EMAIL = "recipient@example.com"
```

### Gmail и пароли приложений

Для Gmail требуется App Password:

- Включить двухфакторную аутентификацию в Google Account.
- Создать пароль приложения.
- Использовать полученный пароль в SMTP_PASS.

`Обычный пароль аккаунта работать не будет.`

### Обработка ошибок

Ошибки SMTP обрабатываются через try/except:

- при недоступности SMTP уведомление выводится в stdout;
- выполнение скрипта не прерывается.

Это сделано осознанно, чтобы не зависеть от внешней инфраструктуры.

## Мини-эссе

Альтернативные технические источники мониторинга

Помимо Википедии, информацию о смерти публичных персон можно отслеживать через новостные API и RSS-ленты крупных СМИ.
Такие источники обеспечивают высокую оперативность, но требуют фильтрации и дедупликации данных.
Более структурированный доступ предоставляют медиа базы данных и событийные платформы, такие как GDELT или
EventRegistry.
Они позволяют получать машиночитаемые данные с классификацией событий, но часто имеют сложную или платную модель
доступа.

Социальные сети могут использоваться как источник первичных сигналов через официальные API и потоковую обработку
сообщений.
Этот подход требует строгой валидации из-за высокого уровня шума и ограничений по rate limit.

Универсальным, но более сложным решением является применение методов обработки естественного языка.
Сюда относятся извлечение именованных сущностей, классификация текстов по типу события и временная корреляция
упоминаний.
Такой подход масштабируем, но требует значительных вычислительных и инженерных ресурсов.

На практике наиболее надёжными являются гибридные архитектуры, комбинирующие несколько источников.
Первичное обнаружение события может выполняться через новости или соцсети, а подтверждение — через Википедию или
официальные сайты.
Это снижает количество ложных срабатываний и повышает достоверность данных.
